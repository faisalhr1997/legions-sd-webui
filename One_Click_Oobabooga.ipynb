{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "mount_file_id": "1glB99Snng4JmxKiFjTisM0lFPYS5XvvZ",
      "authorship_tag": "ABX9TyNakIRKNUocm4+Muf66SSg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/x-legion/legions-sd-notebooks/blob/main/One_Click_Oobabooga.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Keep this tab alive to prevent Colab from disconnecting you { display-mode: \"form\" }\n",
        "\n",
        "#@markdown üéµ`Press play on the music player that will appear below:`\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "R0eIeVNk4taP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.1 Install and Run WebUI { display-mode: \"form\" }\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from IPython.display import clear_output\n",
        "!apt -y update -qq\n",
        "!apt install aria2\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "\n",
        "#@markdown ‚ö†Ô∏è`Google drive is limited to 15GB storage only`\n",
        "everything_on_google_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown `Insert necessary ` [Basic Settings](https://github.com/oobabooga/text-generation-webui/blob/main/README.md#basic-settings) ` in launch_arguments`\n",
        "launch_arguments = \"--share --chat --settings settings-colab-template.json\" #@param [\"--share --chat --settings settings-colab-template.json\"] {allow-input: true}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #**Download Models**\n",
        "#@markdown `Insert any` [HuggingFace](https://huggingface.co/TheBloke) `model repo in <Organization>/<model> format`\n",
        "hf_model_download = \"\" # @param [\"TheBloke/Manticore-13B-Chat-Pyg-Guanaco-SuperHOT-8K-GPTQ\", \"TheBloke/WizardCoder-15B-1.0-GPTQ\", \"TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ\", \"TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQTheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ\", \"TheBloke/OpenAssistant-Llama2-13B-Orca-8K-3319-GPTQ\", \"\"] {allow-input: true}\n",
        "#@markdown `Insert any` [HuggingFace](https://huggingface.co/TheBloke) `model link for a single file download`\n",
        "link_file_download = \"\" #@param [\"https://huggingface.co/TheBloke/orca_mini_3B-GGML/resolve/main/orca-mini-3b.ggmlv3.q4_0.bin\"] {allow-input: true}\n",
        "downloader = \"aria2c\" #@param [\"wget\", \"aria2c\"]\n",
        "\n",
        "\n",
        "# everything on google drive\n",
        "if everything_on_google_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/MyDrive\n",
        "  repo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "  if os.path.exists(repo_dir):\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "  else:\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "\n",
        "else:\n",
        "  %cd /content\n",
        "  repo_dir = '/content/text-generation-webui'\n",
        "  if os.path.exists(repo_dir):\n",
        "    %cd {repo_dir}\n",
        "    !git pull\n",
        "  else:\n",
        "    !git clone https://github.com/oobabooga/text-generation-webui.git\n",
        "\n",
        "\n",
        "%cd {repo_dir}\n",
        "model_dir = f\"{repo_dir}/models\"\n",
        "\n",
        "# hf model download\n",
        "!python download-model.py {hf_model_download}\n",
        "\n",
        "# get filename from url\n",
        "filename = os.path.basename(link_file_download)\n",
        "\n",
        "# wget download\n",
        "if downloader == \"wget\":\n",
        "  %cd {model_dir}\n",
        "  !wget -c --tries=0 --read-timeout=20 --timeout=15 --waitretry=5 --random-wait -r -nH -np -e robots=off -U 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36' -c -O {filename} {link_file_download}\n",
        "# aria2 download\n",
        "if downloader == \"aria2c\":\n",
        "  %cd {model_dir}\n",
        "  !aria2c -x 16 -s 16 -o {filename} {link_file_download} --continue=true --remote-time=true --check-certificate=false --follow-torrent=true\n",
        "\n",
        "\n",
        "# install requirements\n",
        "%cd {repo_dir}\n",
        "!wget https://raw.githubusercontent.com/pcrii/Philo-Colab-Collection/main/settings-colab-template.json -O settings-colab-template.json\n",
        "!pip install -r requirements.txt\n",
        "# !pip install -r /extensions/api/requirements.txt\n",
        "# !pip install -r /extensions/google_translate/requirements.txt\n",
        "# !pip install -r /extensions/ngrok/requirements.txt\n",
        "# !pip install -r /extensions/elevenlabs_tts/requirements.txt\n",
        "# !pip install -r /extensions/openai/requirements.txt\n",
        "# !pip install -r /extensions/silero_tts/requirements.txt\n",
        "# !pip install -r /extensions/superbooga/requirements.txt\n",
        "# !pip install -r /extensions/whisper_stt/requirements.txt\n",
        "\n",
        "!pip uninstall -y llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir\n",
        "clear_output(wait=True)\n",
        "\n",
        "# list models\n",
        "print(\"Models Directory\")\n",
        "print(os.listdir(model_dir))\n",
        "\n",
        "# run webui\n",
        "!python server.py {launch_arguments}"
      ],
      "metadata": {
        "id": "5bazisNevrfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Run WebUI `in case of OOM` { display-mode: \"form\" }\n",
        "\n",
        "!python server.py {launch_arguments}\n"
      ],
      "metadata": {
        "id": "oF-T6SIjTeG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}